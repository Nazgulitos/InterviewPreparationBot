{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing of ios_questions from https://github.com/vyachesIavskiy/iOS-Interview-Questions from README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Load the README content into a string\n",
    "with open('README.md', 'r', encoding='utf-8') as file:\n",
    "    readme_text = file.read()\n",
    "\n",
    "# Define a regex pattern that better handles question and answer separation\n",
    "question_pattern = r'### \\d+\\.\\s*(游릭|游리|游댮)\\s*(.*?)\\n\\n(.*?)(?=\\n\\n### \\d+|$)'\n",
    "\n",
    "# Mapping for levels\n",
    "level_map = {\n",
    "    '游릭': 'Junior',\n",
    "    '游리': 'Middle',\n",
    "    '游댮': 'Senior'\n",
    "}\n",
    "\n",
    "# Initialize list to hold data\n",
    "data = []\n",
    "\n",
    "# Find all matches\n",
    "matches = re.findall(question_pattern, readme_text, re.DOTALL)\n",
    "for match in matches:\n",
    "    level_emoji, question_text, answer_text = match\n",
    "    level = level_map[level_emoji]\n",
    "\n",
    "    # Remove backticks from question text\n",
    "    question_text = question_text.strip().replace(\"`\", \"\")\n",
    "\n",
    "    # Clean up answer text\n",
    "    answer_text = answer_text.strip().replace(\"`\", \"\")\n",
    "\n",
    "    # Append to data list\n",
    "    data.append({\n",
    "        'level': level,\n",
    "        'question': question_text,\n",
    "        'answer': answer_text,\n",
    "        'status': 'none'  # Default status\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('questions_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing of ml_questions from https://www.mygreatlearning.com/blog/machine-learning-interview-questions/#machine-learning-interview-questions-for-experienced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/kokosiknn/opt/anaconda3/envs/myenv/lib/python3.10/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/kokosiknn/opt/anaconda3/envs/myenv/lib/python3.10/site-packages (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kokosiknn/opt/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kokosiknn/opt/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kokosiknn/opt/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kokosiknn/opt/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests) (2023.11.17)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/kokosiknn/opt/anaconda3/envs/myenv/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data parsing complete and saved to ml_interview_questions.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define URLs for each section\n",
    "urls = {\n",
    "    \"Junior\": \"https://www.mygreatlearning.com/blog/machine-learning-interview-questions/#machine-learning-interview-questions-for-freshers\",\n",
    "    \"Middle/Senior\": \"https://www.mygreatlearning.com/blog/machine-learning-interview-questions/#machine-learning-interview-questions-for-experienced\",\n",
    "    \"Deep_Learning\": \"https://www.mygreatlearning.com/blog/machine-learning-interview-questions/#deep-learning-interview-questions\",\n",
    "    \"NLP\": \"https://www.mygreatlearning.com/blog/machine-learning-interview-questions/#nlp-interview-questions\"\n",
    "}\n",
    "\n",
    "# Initialize a list to store the parsed data\n",
    "data = []\n",
    "\n",
    "# Function to parse questions and answers from each URL\n",
    "\n",
    "\n",
    "def parse_questions(url, level):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find all questions by their HTML structure\n",
    "    questions = soup.find_all(\"h3\", class_=\"wp-block-heading\")\n",
    "    for question in questions:\n",
    "        # Extract question text\n",
    "        question_text = question.get_text(strip=True)\n",
    "\n",
    "        # Find the next sibling <p> tag which usually contains the answer\n",
    "        answer_tag = question.find_next_sibling(\"p\")\n",
    "        answer_text = answer_tag.get_text(\n",
    "            strip=True) if answer_tag else \"<br/>\"\n",
    "\n",
    "        question_text = re.sub(r'^\\d+\\.\\s*', '', question_text)\n",
    "\n",
    "        # Append to data list\n",
    "        data.append({\n",
    "            \"level\": level,\n",
    "            \"question\": question_text,\n",
    "            \"answer\": answer_text,\n",
    "            \"status\": \"none\"\n",
    "        })\n",
    "\n",
    "\n",
    "# Parse each section based on the URLs and levels\n",
    "for level, url in urls.items():\n",
    "    parse_questions(url, level)\n",
    "\n",
    "# Convert the parsed data to a DataFrame and save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"ml_interview_questions.csv\", index=False)\n",
    "\n",
    "print(\"Data parsing complete and saved to ml_interview_questions.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "712"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Downloading groq-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/kokosiknn/opt/anaconda3/envs/myenv/lib/python3.10/site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/kokosiknn/opt/anaconda3/envs/myenv/lib/python3.10/site-packages (from groq) (1.9.0)\n",
      "Collecting httpx<1,>=0.23.0 (from groq)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/kokosiknn/opt/anaconda3/envs/myenv/lib/python3.10/site-packages (from groq) (2.6.3)\n",
      "Requirement already satisfied: sniffio in /Users/kokosiknn/opt/anaconda3/envs/myenv/lib/python3.10/site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/kokosiknn/opt/anaconda3/envs/myenv/lib/python3.10/site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/kokosiknn/opt/anaconda3/envs/myenv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (2.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/kokosiknn/opt/anaconda3/envs/myenv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (1.2.0)\n",
      "Requirement already satisfied: certifi in /Users/kokosiknn/opt/anaconda3/envs/myenv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (2023.11.17)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/kokosiknn/opt/anaconda3/envs/myenv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/kokosiknn/opt/anaconda3/envs/myenv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (2.16.3)\n",
      "Downloading groq-0.12.0-py3-none-any.whl (108 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: h11, httpcore, httpx, groq\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.9.0\n",
      "    Uninstalling h11-0.9.0:\n",
      "      Successfully uninstalled h11-0.9.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 0.9.1\n",
      "    Uninstalling httpcore-0.9.1:\n",
      "      Successfully uninstalled httpcore-0.9.1\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.13.3\n",
      "    Uninstalling httpx-0.13.3:\n",
      "      Successfully uninstalled httpx-0.13.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "googletrans 4.0.0rc1 requires httpx==0.13.3, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed groq-0.12.0 h11-0.14.0 httpcore-1.0.7 httpx-0.27.2\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models have revolutionized the field of natural language processing (NLP) by enabling various applications that were previously unimaginable. Here are some reasons why fast language models are crucial:\n",
      "\n",
      "1. **Real-time Applications**: Fast language models enable real-time applications such as chatbots, virtual assistants, and live language translation. These applications require rapid processing and response times to provide a seamless user experience.\n",
      "2. **Efficient Processing**: Fast language models can process large amounts of text data quickly, making them ideal for applications that require handling massive volumes of data, such as social media monitoring, sentiment analysis, and text summarization.\n",
      "3. **Scalability**: Fast language models can be scaled to handle large workloads, making them suitable for applications that require handling a large number of concurrent requests, such as language translation APIs or text analysis platforms.\n",
      "4. **Low Latency**: Fast language models reduce latency, which is critical in applications that require immediate responses, such as virtual assistants, customer service chatbots, or real-time language translation.\n",
      "5. **Improved User Experience**: Fast language models provide a better user experience by responding quickly to user queries, enabling faster decision-making, and improving overall interaction quality.\n",
      "6. **Competitive Advantage**: In today's fast-paced digital landscape, fast language models can provide a competitive advantage by enabling businesses to respond quickly to customer inquiries, analyze large amounts of data, and make data-driven decisions rapidly.\n",
      "7. ** Edge Computing**: Fast language models are essential for edge computing applications, where data is processed closer to the source, reducing latency and improving real-time processing capabilities.\n",
      "8. ** IoT and Autonomous Systems**: Fast language models are critical for IoT and autonomous systems, where rapid processing and response times are necessary for efficient decision-making and real-time control.\n",
      "9. **Research and Development**: Fast language models accelerate research and development in NLP, enabling scientists to explore new ideas, test hypotheses, and iterate faster, leading to breakthroughs in language understanding and generation.\n",
      "10. ** Ethics and Fairness**: Fast language models can help identify biases and ethical concerns in language models, enabling researchers to develop more fair and transparent AI systems.\n",
      "\n",
      "In summary, fast language models are essential for a wide range of applications that require rapid processing, low latency, and efficient handling of large volumes of data. They provide a competitive advantage, improve user experience, and accelerate research and development in NLP.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "GROQ_API_KEY = \"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\"\n",
    "client = Groq(\n",
    "    api_key=GROQ_API_KEY,\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"you are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=False,\n",
    "    stop=None,\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t1.\tk-Nearest Neighbors (k-NN) can also be used for regression problems, where the goal is to predict continuous numerical values rather than categories.\n",
    "\t2.\tIn regression problems, k-NN handles prediction by averaging the response values of the k nearest neighbors to the input observation.\n",
    "\t3.\tThe primary way k-NN classifies an observation is by assigning it to the most common class among its k nearest neighbors, based on a majority vote."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
