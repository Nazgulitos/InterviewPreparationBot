{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n",
      "Precision: 0.96\n",
      "Recall: 1.00\n",
      "F1 Score: 0.98\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('../benchmarking_results/benchmark_results.csv')\n",
    "\n",
    "# Extract the true labels and predicted values\n",
    "y_true = df['is_related']\n",
    "y_pred = [1] * len(df)  # Replace with actual predictions if needed\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved to ../benchmarking_results/updated_answers_2.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Define the input and output file paths\n",
    "input_filename = \"../benchmarking_results/answers_2.csv\"\n",
    "output_filename = \"../benchmarking_results/updated_answers_2.csv\"\n",
    "\n",
    "# Open the input CSV file in read mode\n",
    "with open(input_filename, mode='r', encoding='utf-8') as input_file:\n",
    "    # This reads the file into a dictionary format\n",
    "    reader = csv.DictReader(input_file)\n",
    "    all_rows = []  # To store all rows with the updated llm_score\n",
    "\n",
    "    # Loop through each row in the CSV\n",
    "    for row in reader:\n",
    "        # Check the value of 'llm_score' and convert it to 1 or 0\n",
    "        if row['llm_score'] == '+':\n",
    "            row['llm_score'] = '1'\n",
    "        else:\n",
    "            row['llm_score'] = '0'\n",
    "\n",
    "        # Append the updated row to the list\n",
    "        all_rows.append(row)\n",
    "\n",
    "# Define the fieldnames (headers) for the new CSV\n",
    "fieldnames = reader.fieldnames  # Use the same headers as the input file\n",
    "\n",
    "# Open the output CSV file in write mode\n",
    "with open(output_filename, mode='w', newline='', encoding='utf-8') as output_file:\n",
    "    writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "\n",
    "    # Write the header\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Write all the rows with the updated llm_score\n",
    "    writer.writerows(all_rows)\n",
    "\n",
    "print(f\"Updated CSV saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6042\n",
      "Precision: 0.6364\n",
      "Recall: 0.5600\n",
      "F1 Score: 0.5957\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Define the input file path\n",
    "input_filename = \"../benchmarking_results/updated_answers_2.csv\"\n",
    "\n",
    "# Initialize counters for TP, TN, FP, FN\n",
    "tp = tn = fp = fn = 0\n",
    "\n",
    "# Open the CSV file in read mode\n",
    "with open(input_filename, mode='r', encoding='utf-8') as input_file:\n",
    "    reader = csv.DictReader(input_file)\n",
    "\n",
    "    # Loop through each row in the CSV\n",
    "    for row in reader:\n",
    "        y_true = int(row['y_true'])  # Convert to integer (1 or 0)\n",
    "        llm_score = int(row['llm_score'])  # Convert to integer (1 or 0)\n",
    "\n",
    "        # Calculate TP, TN, FP, FN\n",
    "        if y_true == 1 and llm_score == 1:\n",
    "            tp += 1\n",
    "        elif y_true == 0 and llm_score == 0:\n",
    "            tn += 1\n",
    "        elif y_true == 0 and llm_score == 1:\n",
    "            fp += 1\n",
    "        elif y_true == 1 and llm_score == 0:\n",
    "            fn += 1\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision +\n",
    "                                 recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
